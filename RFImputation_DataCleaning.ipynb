{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from scipy.stats import pearsonr\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "#Import Random Forest Model\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "#Import scikit-learn metrics module for accuracy calculation\n",
    "from sklearn import metrics\n",
    "\n",
    "import re\n",
    "import math\n",
    "\n",
    "# import dependencies\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display, HTML"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1: Grabbing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(s): #fxn for converting one of the target variables to int\n",
    "    if s == 'Excellent,':\n",
    "        return 5\n",
    "    elif s == 'Very good,':\n",
    "        return 4\n",
    "    elif s=='Good,':\n",
    "        return 3\n",
    "    elif s=='Fair, or':\n",
    "        return 2\n",
    "    elif s=='Poor?':\n",
    "        return 1\n",
    "    else:\n",
    "        return s\n",
    "\n",
    "def create_pca(train_df,target_df):\n",
    "    subset = train_df.copy()\n",
    "    target_cols_df = target_df.copy()\n",
    "\n",
    "    #split up dataset subset and target\n",
    "    x_train, x_test, y_train, y_test = train_test_split(subset, target_cols_df, test_size=1/7.0, random_state=0)\n",
    "\n",
    "    ##Create PCA\n",
    "    scaler = StandardScaler()\n",
    "\n",
    "    # Fit on training set only.\n",
    "    scaler.fit(x_train)\n",
    "    # Apply transform to both the training set and the test set.\n",
    "    train = scaler.transform(x_train)\n",
    "    test = scaler.transform(x_test)\n",
    "\n",
    "    # Make an instance of the Model. Capturing 95% of the variance\n",
    "    pca = PCA(.95)\n",
    "\n",
    "    pca.fit(train)\n",
    "\n",
    "    pca_train = pca.transform(train)\n",
    "    pca_test = pca.transform(test)\n",
    "\n",
    "    #look at the PCA data and find the top least important variables\n",
    "    pca_df = pd.DataFrame(pca.components_,columns=subset.columns)\n",
    "\n",
    "    return pca_train, pca_test, y_train, y_test, pca_df, [pca, scaler]\n",
    "\n",
    "def create_rf(pca_train, pca_test, y_train, n_estimators =100 ):\n",
    "    #Create a Gaussian Classifier\n",
    "    rf=RandomForestClassifier(n_estimators)\n",
    "\n",
    "    #Train the model using the training sets y_pred=clf.predict(X_test)\n",
    "    rf.fit(pca_train,y_train)\n",
    "\n",
    "    y_pred=rf.predict(pca_test)\n",
    "\n",
    "    return rf, y_pred\n",
    "def group_age(x):\n",
    "\n",
    "    try:\n",
    "        v = int(x)\n",
    "    except:\n",
    "        try:\n",
    "            if math.isnan(x):\n",
    "                return 0\n",
    "        except:\n",
    "            return '80+'\n",
    "    \n",
    "    if v < 18:\n",
    "        return '17-'\n",
    "    elif v >= 18 and v < 30:\n",
    "        return '18-29'\n",
    "    elif v >= 30 and v < 40:\n",
    "        return '30-39'\n",
    "    elif v >= 40 and v < 50:\n",
    "        return '40-49'\n",
    "    elif v >= 50 and v < 60:\n",
    "        return '50-59'\n",
    "    elif v >= 60 and v < 70:\n",
    "        return '60-69'\n",
    "    elif v >= 70 and v < 80:\n",
    "        return '70-79'\n",
    "    elif int(x) >= 80:\n",
    "        return '80+'\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "def group_edu(x):\n",
    "    ged = 'GED'\n",
    "    no_dip = 'No Diploma|no diploma'\n",
    "    college = 'More than high school'\n",
    "    idk = 'Don\\'t Know'\n",
    "    hs = 'High school graduate|High School Graduate'\n",
    "    num = '\\d+'\n",
    "\n",
    "    if type(x) == float:\n",
    "        return 'Other'\n",
    "\n",
    "    if re.search(ged, x):\n",
    "        return 'GED'\n",
    "    elif re.search(no_dip,x):\n",
    "        return 'High School'\n",
    "    elif re.search(college,x):\n",
    "        return 'College'\n",
    "    elif re.search(idk, x):\n",
    "        return 'Other'\n",
    "    elif re.search(hs,x):\n",
    "        return 'HS Grad'\n",
    "    elif re.search(num,x):\n",
    "        a = re.search(num,x)\n",
    "        ind1 = a.span()[0]\n",
    "        ind2 = a.span()[1]\n",
    "        grade = int(x[ind1])\n",
    "\n",
    "        if ind2 > 1:\n",
    "            return 'Grade School'\n",
    "        elif grade >= 9:\n",
    "            return 'High School'\n",
    "        elif grade < 9:\n",
    "            return 'Grade School'\n",
    "    else:\n",
    "        return x\n",
    "\n",
    "def group_pir(x):\n",
    "    if type(x) == float:\n",
    "        return 'Other'\n",
    "    elif re.search('greater', x):\n",
    "        return '5+'\n",
    "    else:\n",
    "        return x\n",
    "\n",
    "#raw_data_filled['RIDAGEYR'] = raw_data_filled['RIDAGEYR'].apply(lambda x: group_age(x) )\n",
    "#raw_data_filled['DMDEDUC3'] = raw_data_filled['DMDEDUC3'].apply(lambda x: group_edu(x) )\n",
    "#raw_data_filled['INDFMPIR'] = raw_data_filled['INDFMPIR'].apply(lambda x: group_pir(x) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-4-deef3e584cc6>:3: DtypeWarning: Columns (27,41,77) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  raw_data_df = pd.read_csv('/Users/kevin/Desktop/OMSA/CSE6242/Project/Project Visual/all_nhanes_filtered.csv')\n"
     ]
    }
   ],
   "source": [
    "##creating Train_df1 and target_df\n",
    "#grabbing dataset\n",
    "raw_data_df = pd.read_csv('/Users/kevin/Desktop/OMSA/CSE6242/Project/Project Visual/all_nhanes_filtered.csv')\n",
    "\n",
    "#replace don't know with NULL\n",
    "raw_data_df  = raw_data_df.replace(re.compile('(Don\\'t know|Refused)'), float('NaN'))\n",
    "\n",
    "#specifying target columns\n",
    "target_cols = ['MCQ220','MCQ160E','MCQ160F','HUQ010' ]\n",
    "\n",
    "#Getting non cat columns\n",
    "non_cat_cols = []\n",
    "for c in raw_data_df.columns:\n",
    "    if raw_data_df[c].dtype != object:\n",
    "        non_cat_cols.append(c)\n",
    "\n",
    "#put this after grabbing non-cat cols to avoid grabbing target variables\n",
    "raw_data_df = raw_data_df.replace(\"Yes\", 1).replace(\"No\", 2).replace(9,float('NaN')).replace(7,float('NaN')) \\\n",
    "                        .replace('9',float('NaN')).replace('7',float('NaN'))#.drop(['SEQN'], axis =1 )\n",
    "\n",
    "raw_data_df['RIDAGEYR'] = raw_data_df['RIDAGEYR'].apply(lambda x: group_age(x) )\n",
    "raw_data_df['DMDEDUC3'] = raw_data_df['DMDEDUC3'].apply(lambda x: group_edu(x) )\n",
    "raw_data_df['INDFMPIR'] = raw_data_df['INDFMPIR'].apply(lambda x: group_pir(x) )\n",
    "\n",
    "#keeping only non cat columns and filling in answers\n",
    "subset_df = raw_data_df.copy()\n",
    "subset_df = subset_df[non_cat_cols].fillna(0)\n",
    "\n",
    "df_dict= {}#this is a dict which holds the dfs to train on for each target variable\n",
    "#for each target variable, remove rows in entire df whenever there is a NULL or Don't Know\n",
    "for col in target_cols:\n",
    "    t_c = raw_data_df[col]\n",
    "    if col == 'HUQ010':\n",
    "        t_c =  t_c.apply(lambda x : f(x))\n",
    "\n",
    "    df = pd.concat([subset_df.copy(), t_c ], axis = 1) #merging dataset and target variable\n",
    "    df = df[df[col].notna()]\n",
    "    df[col] = df[col].astype(int)\n",
    "    df_dict[col] = df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Year_Start</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MCQ160F</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2120</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>51022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>78</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1806</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Year_Start\n",
       "MCQ160F            \n",
       "1              2120\n",
       "2             51022\n",
       "1                78\n",
       "2              1806"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_data_df[['Year_Start','MCQ160F']].groupby('MCQ160F').count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2: Training RF Models Based on Non-Null Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_dict = {} # this is a dict of all of the correlations for each target variable\n",
    "\n",
    "for col in target_cols:\n",
    "    corr_dict[col] = df_dict[col].corr()[col]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "#find the top 25 variables per target variable, highest positive or negative correlations\n",
    "top_25 = pd.DataFrame()\n",
    "\n",
    "for col in target_cols:\n",
    "    d = corr_dict[col].sort_values(ascending = False, key = abs).head(25).reset_index()\n",
    "    d= d.rename(columns = {'index':str(col) +'var'})\n",
    "    d = d.drop(labels=[0], axis = 0).reset_index(drop = True) #need to remove the first row bc that row is the var itself\n",
    "    top_25 = pd.concat([top_25,d], axis = 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-184-760c23f7b8d3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpca_dict\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m     \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_rf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'pca_train'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'pca_test'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'y_train'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_estimators\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m     \u001b[0mrf_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m'model'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'y_pred'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'pca'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mpca_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-180-3bbfb5a86bc8>\u001b[0m in \u001b[0;36mcreate_rf\u001b[0;34m(pca_train, pca_test, y_train, n_estimators)\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m     \u001b[0;31m#Train the model using the training sets y_pred=clf.predict(X_test)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m     \u001b[0mrf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpca_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m     \u001b[0my_pred\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpca_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/sklearn/ensemble/_forest.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    385\u001b[0m             \u001b[0;31m# parallel_backend contexts set at a higher level,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    386\u001b[0m             \u001b[0;31m# since correctness does not rely on using threads.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 387\u001b[0;31m             trees = Parallel(n_jobs=self.n_jobs, verbose=self.verbose,\n\u001b[0m\u001b[1;32m    388\u001b[0m                              \u001b[0;34m**\u001b[0m\u001b[0m_joblib_parallel_args\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprefer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'threads'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    389\u001b[0m                 delayed(_parallel_build_trees)(\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1042\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_iterator\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1043\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1044\u001b[0;31m             \u001b[0;32mwhile\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdispatch_one_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1045\u001b[0m                 \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1046\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36mdispatch_one_batch\u001b[0;34m(self, iterator)\u001b[0m\n\u001b[1;32m    857\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    858\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 859\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dispatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtasks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    860\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    861\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m_dispatch\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    775\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    776\u001b[0m             \u001b[0mjob_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 777\u001b[0;31m             \u001b[0mjob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    778\u001b[0m             \u001b[0;31m# A job can complete so quickly than its callback is\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    779\u001b[0m             \u001b[0;31m# called before we get here, causing self._jobs to\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36mapply_async\u001b[0;34m(self, func, callback)\u001b[0m\n\u001b[1;32m    206\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mapply_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    207\u001b[0m         \u001b[0;34m\"\"\"Schedule a func to be run\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 208\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImmediateResult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    209\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    210\u001b[0m             \u001b[0mcallback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    570\u001b[0m         \u001b[0;31m# Don't delay the application, to avoid keeping the input\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    571\u001b[0m         \u001b[0;31m# arguments in memory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 572\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    573\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    574\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    260\u001b[0m         \u001b[0;31m# change the default number of processes to -1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    261\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mparallel_backend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_n_jobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 262\u001b[0;31m             return [func(*args, **kwargs)\n\u001b[0m\u001b[1;32m    263\u001b[0m                     for func, args, kwargs in self.items]\n\u001b[1;32m    264\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    260\u001b[0m         \u001b[0;31m# change the default number of processes to -1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    261\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mparallel_backend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_n_jobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 262\u001b[0;31m             return [func(*args, **kwargs)\n\u001b[0m\u001b[1;32m    263\u001b[0m                     for func, args, kwargs in self.items]\n\u001b[1;32m    264\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/sklearn/utils/fixes.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    220\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    221\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mconfig_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 222\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/sklearn/ensemble/_forest.py\u001b[0m in \u001b[0;36m_parallel_build_trees\u001b[0;34m(tree, forest, X, y, sample_weight, tree_idx, n_trees, verbose, class_weight, n_samples_bootstrap)\u001b[0m\n\u001b[1;32m    167\u001b[0m                                                         indices=indices)\n\u001b[1;32m    168\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 169\u001b[0;31m         \u001b[0mtree\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcurr_sample_weight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcheck_input\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    170\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    171\u001b[0m         \u001b[0mtree\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcheck_input\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/sklearn/tree/_classes.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight, check_input, X_idx_sorted)\u001b[0m\n\u001b[1;32m    896\u001b[0m         \"\"\"\n\u001b[1;32m    897\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 898\u001b[0;31m         super().fit(\n\u001b[0m\u001b[1;32m    899\u001b[0m             \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    900\u001b[0m             \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/sklearn/tree/_classes.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight, check_input, X_idx_sorted)\u001b[0m\n\u001b[1;32m    387\u001b[0m                                            min_impurity_split)\n\u001b[1;32m    388\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 389\u001b[0;31m         \u001b[0mbuilder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtree_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    390\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    391\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_outputs_\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mis_classifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "### PCA LOOP STARTS HERE\n",
    "pca_dict = {} #dict that holds all of the trained PCA models for each target variable\n",
    "\n",
    "for col in target_cols:\n",
    "    var_names = top_25[str(col) + 'var'] #getting top 25 variables for each target variable\n",
    "    \n",
    "    train = df_dict[col][var_names]\n",
    "    target = df_dict[col][col]\n",
    "    \n",
    "    #create PCA to evaluate the least important variables\n",
    "    pca_train, pca_test, y_train, y_test, pca_df, pca_model = create_pca(train,target)\n",
    "\n",
    "    #For each variable, look at how many times it has a value contributing to less than .1 of the variance for that principle component\n",
    "    counts = {}\n",
    "    for (columnName, columnData) in pca_df.iteritems():\n",
    "        count = 0\n",
    "        for v in columnData.values:\n",
    "            if v<.1:\n",
    "                count += 1\n",
    "        counts[columnName] = count\n",
    "    max_counts = max(counts.values())\n",
    "\n",
    "    #Grab the top offending variables and remove them from names\n",
    "    remove_counts = {k:v for k,v in counts.items() if v  == max_counts }\n",
    "    keep_names = [name for name in var_names if name not in remove_counts]\n",
    "\n",
    "    #run PCA again with reduced variables\n",
    "    train = df_dict[col][keep_names]\n",
    "    pca_train, pca_test, y_train, y_test, pca_df, pca_model = create_pca(train,target)\n",
    "\n",
    "    pca_dict[col] = {'pca_train': pca_train, 'pca_test': pca_test, \n",
    "                    'y_train': y_train, 'y_test': y_test,\n",
    "                    'pca_df': pca_df,'keep_names': keep_names,\n",
    "                    'model': pca_model}\n",
    "\n",
    "### RF MODEL LOOP STARTS HERE\n",
    "rf_dict = {} #dict that holds all of the RF models + pca_dict entry for each target variable\n",
    "\n",
    "for k,v in pca_dict.items():\n",
    "    model, y_pred = create_rf(v['pca_train'], v['pca_test'], v['y_train'], n_estimators = 100)\n",
    "    rf_dict[k] = {'model': model, 'y_pred':y_pred, 'pca':pca_dict[k]}\n",
    "\n",
    "for k,v in rf_dict.items():\n",
    "    print(str(k) + \" Accuracy:\",metrics.accuracy_score(v['pca']['y_test'], v['y_pred']))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 3: Fill in Missing Data Using RF Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rf_predict(rf_model, pca_model, row):\n",
    "     pca = pca_model[0]\n",
    "     scaler = pca_model[1]\n",
    "     train = scaler.transform(row)\n",
    "     train = pca.transform(train)\n",
    "\n",
    "     return rf_model.predict(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########### THIS BLOCK TAKES 25MIN TO RUN ####################\n",
    "\n",
    "filled_dict= {}#this is a dict which holds the dfs with filled in values for null\n",
    "\n",
    "for col in target_cols:\n",
    "    t_c = raw_data_df[col]\n",
    "    if col == 'HUQ010':\n",
    "        t_c =  t_c.apply(lambda x : f(x))\n",
    "    df = pd.concat([subset_df.copy()[rf_dict[col]['pca']['keep_names']], t_c ], axis = 1) #merging dataset and target variable\n",
    "    df = df.replace(\"Yes\", 1) .replace(\"No\", 0)\n",
    "    \n",
    "\n",
    "    for ind, row in df.iterrows():\n",
    "        try:\n",
    "            check = int(row[-1])\n",
    "        except:\n",
    "            check = row[-1]\n",
    "        try:\n",
    "            if math.isnan(check):\n",
    "                row_df = df.iloc[[ind],:-1]\n",
    "                ans = rf_predict(rf_dict[col]['model'],rf_dict[col]['pca']['model'], row_df) #inputting every column except for the last(target)\n",
    "                df.at[ind,col] = ans[0]\n",
    "        except:\n",
    "            print('offending ', row[-1])\n",
    "\n",
    "    df[col] = df[col].astype(int)\n",
    "    filled_dict[col] = df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Add back Year column to each dataframe in filled_dict\n",
    "\n",
    "graph_dict = {} ##dict holding all DFs to be graphed\n",
    "year_df = raw_data_df[['Year_Start']].copy()\n",
    "\n",
    "for k,v in filled_dict.items():\n",
    "    filled = pd.concat([year_df, v[k]], axis = 1)\n",
    "    unfilled = df_dict[k][['Year_Start',k]]\n",
    "    graph_dict[k] = {'filled':filled, 'unfilled':unfilled}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MCQ220 [2 1]\n",
      "MCQ160E [2 1]\n",
      "MCQ160F [2 1]\n",
      "HUQ010 [5 4 3 2 1]\n"
     ]
    }
   ],
   "source": [
    "for col in target_cols:\n",
    "    print(col, graph_dict[col]['filled'][col].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Getting Summary Statistics for each columns. May not end up using this\n",
    "summed_dict = {}\n",
    "\n",
    "for col in target_cols:\n",
    "    df_filled = graph_dict[col]['filled'].groupby(['Year_Start', col]).size().to_frame().reset_index()\n",
    "    df_filled = df_filled.rename(columns = {col:'answer',0:'count'})\n",
    "    df_sum = graph_dict[col]['filled'].groupby(['Year_Start']).count().reset_index()\n",
    "    df_sum = df_sum.rename(columns = {col:'total_count'})\n",
    "    df_filled = df_filled.merge(df_sum, how = 'inner', on = 'Year_Start')\n",
    "    df_filled['pct'] = df_filled['count']/df_filled['total_count']\n",
    "\n",
    "    df_unfilled = graph_dict[col]['unfilled'].groupby(['Year_Start', col]).size().to_frame().reset_index()\n",
    "    df_unfilled = df_unfilled.rename(columns = {col:'answer',0:'count'})\n",
    "    df_sum = graph_dict[col]['unfilled'].groupby(['Year_Start']).count().reset_index()\n",
    "    df_sum = df_sum.rename(columns = {col:'total_count'})\n",
    "    df_unfilled = df_unfilled.merge(df_sum, how = 'inner', on = 'Year_Start')\n",
    "    df_unfilled['pct'] = df_unfilled['count']/df_unfilled['total_count']\n",
    "\n",
    "    summed_dict[col] = {'filled': df_filled, 'unfilled' : df_unfilled}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pivot_dict = {}\n",
    "\n",
    "for col in target_cols:\n",
    "    df = summed_dict[col]['filled']\n",
    "    df = df.pivot(columns = 'answer', index = 'Year_Start')\n",
    "\n",
    "    du = summed_dict[col]['unfilled']\n",
    "    du = du.pivot(columns = 'answer', index = 'Year_Start')\n",
    "\n",
    "    pivot_dict[col] = {'filled':df, 'unfilled': du}\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###Save datframe to pickle file\n",
    "# try:\n",
    "#     import pickle as pickle\n",
    "# except ImportError:  # Python 3.x\n",
    "#     import pickle\n",
    "\n",
    "# with open('rf_dict.p', 'wb') as fp:\n",
    "#     pickle.dump(rf_dict, fp, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "###Load pickle\n",
    "# with open('pivot_data.p', 'rb') as fp:\n",
    "#     data = pickle.load(fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(raw_data_df.columns, columns = {'Variable'})\n",
    "var_desc_df = pd.read_csv('/Users/kevin/Desktop/OMSA/CSE6242/Project/Project Visual/VariableDescriptions.csv')\n",
    "desc_df = df.merge(var_desc_df, how = \"left\", on = 'Variable')\n",
    "\n",
    "desc_df.to_csv('/Users/kevin/Desktop/OMSA/CSE6242/Project/Project Visual/DescriptionsDF.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating a copy of raw data df that has the filled in values for our target variables\n",
    "raw_data_filled = raw_data_df.copy()\n",
    "\n",
    "for col in target_cols:\n",
    "    raw_data_filled[col + '_filled'] = filled_dict[col][col]\n",
    "\n",
    "for col in target_cols:\n",
    "    raw_data_filled = raw_data_filled.rename(columns = {col: col+'_unfilled'})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data_filled.to_csv('/Users/kevin/Desktop/OMSA/CSE6242/Project/Project Visual/raw_data_filled.csv')\n",
    "\n",
    "#Finish group_edu, then start graphing by demo and by age and education"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Code Below is from experimenting and prior data explorations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def plot_chart(col, year):\n",
    "#     # Create figure and axis #1\n",
    "#     fig, (ax1, ax3) = plt.subplots(1,2, figsize=(15, 5))\n",
    "\n",
    "#     # x axis values and ticks\n",
    "#     year_list = list(pivot_dict[col]['filled'].index)\n",
    "#     idx = year_list.index(year) \n",
    "#     xtick = year_list[idx:]\n",
    "#     xpct = year_list[idx:]\n",
    "#     #for all other variables\n",
    "#     xbarl = [i-.2 for i in year_list[idx:] ] \n",
    "#     xbarr = [i+.2 for i in year_list[idx:] ] \n",
    "#     #for H variable\n",
    "#     hb1 = [i-.4 for i in year_list[idx:] ] \n",
    "#     hb2 = [i-.2 for i in year_list[idx:] ] \n",
    "#     hb3 = [i for i in year_list[idx:] ] \n",
    "#     hb4 = [i+.2 for i in year_list[idx:] ] \n",
    "#     hb5 = [i+.4 for i in year_list[idx:] ] \n",
    "\n",
    "#     #set titles\n",
    "#     ax1.set_title('Predicted_Answers' + \" \" + col)\n",
    "#     ax3.set_title('Non-Null_Answers' + \" \" + col)\n",
    "\n",
    "#     ax3.set_axisbelow(True)\n",
    "\n",
    "#     ### LEFT PLOT\n",
    "#     # plot line chart on axis #1\n",
    "#     if col != 'HUQ010':\n",
    "#         ax1.plot(xpct, pivot_dict[col]['filled']['pct'][1].loc[year:], alpha=.75, color='orange') \n",
    "#         ax1.plot(xpct, pivot_dict[col]['filled']['pct'][2].loc[year:], alpha=.75, color='blue') \n",
    "#     ax1.set_ylabel('%')\n",
    "#     ax1.set_ylim(0, 1.5)\n",
    "#     ax1.set_xticks(xtick)\n",
    "\n",
    "#     # set up twin(shared) axis\n",
    "#     ax2 = ax1.twinx()\n",
    "\n",
    "#     # plot bar chart on axis #2\n",
    "#     if col == 'HUQ010':\n",
    "    \n",
    "#         ax2.bar(hb1, pivot_dict['HUQ010']['filled']['count'].loc[year:,:].reset_index(drop=True)[1], width=0.25, alpha=0.75, color='red', zorder = 2)\n",
    "#         ax2.bar(hb2, pivot_dict['HUQ010']['filled']['count'].loc[year:,:].reset_index(drop=True)[2], width=0.25, alpha=0.75, color='blue', zorder = 2)\n",
    "#         ax2.bar(hb3, pivot_dict['HUQ010']['filled']['count'].loc[year:,:].reset_index(drop=True)[3], width=0.25, alpha=0.75, color='green', zorder = 2)\n",
    "#         ax2.bar(hb4, pivot_dict['HUQ010']['filled']['count'].loc[year:,:].reset_index(drop=True)[4], width=0.25, alpha=0.75, color='orange', zorder = 2)\n",
    "#         ax2.bar(hb5, pivot_dict['HUQ010']['filled']['count'].loc[year:,:].reset_index(drop=True)[5], width=0.25, alpha=0.75, color='brown', zorder = 2)\n",
    "        \n",
    "#         ax2.legend(['1','2','3','4','5'], loc=\"upper right\")\n",
    "#     else:\n",
    "#         ax1.legend(['Pct No', 'Pct Yes'], loc=\"upper left\")\n",
    "#         ax2.legend(['No', 'Yes'], loc=\"upper right\")\n",
    "        \n",
    "#         ax2.bar(xbarl, pivot_dict[col]['filled']['count'][1].loc[year:], width=0.5, alpha=0.75, color='orange', zorder = 2)\n",
    "#         ax2.bar(xbarr, pivot_dict[col]['filled']['count'][2].loc[year:], width=0.5, alpha=0.75, color='blue', zorder = 2)\n",
    "\n",
    "#     # ax2.grid(zorder = 0)\n",
    "#     ax2.set_ylim(0, 15000)\n",
    "#     ax2.set_ylabel('total_answers')\n",
    "#     ###\n",
    "\n",
    "#     ### RIGHT PLOT\n",
    "#     # plot line chart on axis #3\n",
    "#     if col != 'HUQ010':\n",
    "#         ax3.plot(xpct, pivot_dict[col]['unfilled']['pct'][1].loc[year:], alpha=.75, color='orange') \n",
    "#         ax3.plot(xpct, pivot_dict[col]['unfilled']['pct'][2].loc[year:], alpha=.75, color='blue')\n",
    "    \n",
    "#     ax3.set_ylabel('%')\n",
    "#     ax3.set_ylim(0, 1.5) \n",
    "#     ax3.set_xticks(xtick)\n",
    "\n",
    "\n",
    "#     # set up twin(shared) axis\n",
    "#     ax4 = ax3.twinx()\n",
    "\n",
    "#     # plot bar chart on axis #4\n",
    "#     if col == 'HUQ010':\n",
    "#         ax4.bar(hb1, pivot_dict['HUQ010']['unfilled']['count'].loc[year:,:].reset_index(drop=True)[1], width=0.25, alpha=0.75, color='red', zorder = 2)\n",
    "#         ax4.bar(hb2, pivot_dict['HUQ010']['unfilled']['count'].loc[year:,:].reset_index(drop=True)[2], width=0.25, alpha=0.75, color='blue', zorder = 2)\n",
    "#         ax4.bar(hb3, pivot_dict['HUQ010']['unfilled']['count'].loc[year:,:].reset_index(drop=True)[3], width=0.25, alpha=0.75, color='green', zorder = 2)\n",
    "#         ax4.bar(hb4, pivot_dict['HUQ010']['unfilled']['count'].loc[year:,:].reset_index(drop=True)[4], width=0.25, alpha=0.75, color='orange', zorder = 2)\n",
    "#         ax4.bar(hb5, pivot_dict['HUQ010']['unfilled']['count'].loc[year:,:].reset_index(drop=True)[5], width=0.25, alpha=0.75, color='brown', zorder = 2)\n",
    "\n",
    "#         ax4.legend(['1','2','3','4','5'], loc=\"upper right\")\n",
    "#     else:\n",
    "#         ax3.legend(['Pct No', 'Pct Yes'], loc=\"upper left\")\n",
    "        \n",
    "#         ax4.bar(xbarl, pivot_dict[col]['unfilled']['count'][1].loc[year:], width=0.5, alpha=.75, color='orange', zorder = 2)\n",
    "#         ax4.bar(xbarr, pivot_dict[col]['unfilled']['count'][2].loc[year:], width=0.5, alpha=.75, color='blue', zorder = 2)\n",
    "#         ax4.legend(['No','Yes'], loc=\"upper right\")\n",
    "\n",
    "#     # ax4.grid(False, zorder=0) # turn off grid #2\n",
    "#     ax4.set_ylim(0, 15000)\n",
    "#     ax4.set_ylabel('total_answers')\n",
    "#     #legend\n",
    "    \n",
    "#     ###\n",
    "\n",
    "#     #spacing the plots apart\n",
    "#     fig.tight_layout(pad = 3)\n",
    "#     plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #dropdownlist variables\n",
    "# years = list(pivot_dict[col]['filled'].index)\n",
    "# variables = target_cols.copy()\n",
    "# #years.insert(0,'All')\n",
    "# years.insert(0,'Choose')\n",
    "# variables.insert(0,'Choose')\n",
    "\n",
    "# #creating widget objects\n",
    "# output = widgets.Output()\n",
    "# dropdown_variable = widgets.Dropdown(options = variables, description = 'Variable')\n",
    "# dropdown_year = widgets.Dropdown(options = years, description = 'Year')\n",
    "\n",
    "\n",
    "# #function for handling event\n",
    "# def common_filtering(variable = None, year = None):\n",
    "#     output.clear_output()\n",
    "#     if variable != 'Choose' and year != 'Choose':\n",
    "#         with output:\n",
    "#             plot_chart(variable, year)\n",
    "\n",
    "# #event handler functions\n",
    "# def dropdown_variable_eventhandler(change):\n",
    "#     common_filtering(change.new, dropdown_year.value)\n",
    "\n",
    "# def dropdown_year_eventhandler(change):\n",
    "#     common_filtering(dropdown_variable.value, change.new)\n",
    "\n",
    "# dropdown_variable.observe(dropdown_variable_eventhandler, names='value')\n",
    "# dropdown_year.observe(dropdown_year_eventhandler, names='value')\n",
    "\n",
    "# display(dropdown_variable)\n",
    "# display(dropdown_year)\n",
    "# display(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# #dataset variable descriptions\n",
    "# var_desc_df = pd.read_csv('/Users/kevin/Desktop/OMSA/CSE6242/Project/Project Visual/VariableDescriptions.csv')\n",
    "\n",
    "\n",
    "# #creating df with only cat cols and desc\n",
    "# cat_df = pd.DataFrame(data = cat_cols, columns = ['Variable'])\n",
    "# cat_df = cat_df.merge(var_desc_df, how = \"left\", on = 'Variable')\n",
    "# keep_cat_cols = ['RIAGENDR','HUQ010','MCQ160F','MCQ160E','MCQ160C','MCQ220']\n",
    "\n",
    "\n",
    "# #creating df with all cols and desc\n",
    "# col_df = pd.DataFrame(data = data_df.columns, columns = ['Variable'])\n",
    "# col_df = col_df.merge(var_desc_df, how = \"left\", on = 'Variable')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# #remove sequence id col\n",
    "# print('\\n')\n",
    "# for c in target_cols_df.columns:\n",
    "#     print(c)\n",
    "#     print(target_cols_df[c].dtype)\n",
    "#     print(target_cols_df[c].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # corr, _ = pearsonr(df_final[], data2)\n",
    "# #getting correlation matrix for all variable\n",
    "# corr_df = df_final.corr()\n",
    "# corr_df.head()\n",
    "\n",
    "# corr_df.to_csv('/Users/kevin/Desktop/OMSA/CSE6242/Project/Project Visual/test.csv')\n",
    "# cat_vars = ['RIDEXPRG','MCQ010','MCQ160C','MCQ160E','MCQ160F','MCQ220',\n",
    "#             'VTQ200A','KIQ201','VTQ280A','VTQ280B','VTQ280C','VTQ280D',\n",
    "#             'VTQ280E','VTQ280F','VTQ280G','VTQ280H','VTQ281A','VTQ281C','VTQ281E']\n",
    "\n",
    "# d = corr_df.filter(items = cat_vars, axis=0)\n",
    "# d.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #finding highly correlated variables\n",
    "# threshold = [.5, -.5] \n",
    "# top_pairs = []\n",
    "# for row_ind, row in corr_df.iterrows():\n",
    "#     for ind, value in row.items():\n",
    "#         if (value > threshold[0] or value < threshold[1]) and (row_ind != ind) :\n",
    "#             a = [row_ind, ind]\n",
    "#             a.sort()\n",
    "#             if a not in top_pairs:\n",
    "#                 top_pairs.append(a)\n",
    "\n",
    "# print(top_pairs[:10])\n",
    "# print(len(top_pairs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #getting variable descriptions\n",
    "# top_pairs_df = pd.DataFrame(data = top_pairs, columns = ['p1','p2'])\n",
    "# merged_df = top_pairs_df.merge(var_desc_df, how='left',left_on='p1',right_on='Variable').merge(var_desc_df, how = 'left', left_on = 'p2',right_on = 'Variable')\n",
    "# merged_df = merged_df[['p1','p2','SAS_Label_x','SAS_Label_y']].rename(columns = {'SAS_Label_x':'p1_label','SAS_Label_y':'p2_label'})\n",
    "# #adding correlation\n",
    "# merged_df['corr'] = merged_df[['p1','p2']].apply(lambda x: corr_df.loc[x.p1,x.p2], axis=1)\n",
    "# merged_df = merged_df[['p1','p2','corr','p1_label','p2_label']].sort_values(['p1','corr'], ascending= [True, False], ignore_index=True)\n",
    "# merged_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merged_df.to_csv('/Users/kevin/Desktop/OMSA/CSE6242/Project/Project Visual/top_pairs.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Counting how many times each variable shows up\n",
    "# pair_count = {}\n",
    "# for pair in top_pairs:\n",
    "#     for item in pair:\n",
    "#         if item not in pair_count.keys():\n",
    "#             pair_count[item] = 1\n",
    "#         else:\n",
    "#             pair_count[item] += 1\n",
    "\n",
    "# #sorting from greatest to least\n",
    "# top_pair_count_sorted = sorted(pair_count.items(), key=lambda kv:\n",
    "#                  (kv[1], kv[0]), reverse = True)\n",
    "\n",
    "# max_value = max(pair_count.values())\n",
    "# top_pair_count_sorted[:10]\n",
    "\n",
    "# #Grabbing the most commonly shown up variables within the highly correlated pairs of \n",
    "# top_vars = []\n",
    "# for pair in top_pair_count_sorted:\n",
    "#     if pair[1] == max_value:\n",
    "#         top_vars.append(pair[0])\n",
    "\n",
    "# top_vars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# top_vars_dict = {var:[] for var in top_vars}\n",
    "\n",
    "# for pair in pairs:\n",
    "#     if pair[0] in top_vars:\n",
    "#         top_vars_dict[pair[0]].append(pair)\n",
    "#     if pair[1] in top_vars:\n",
    "#         top_vars_dict[pair[1]].append(pair)\n",
    "\n",
    "#top_vars_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_final_top = df_final[top_vars]\n",
    "\n",
    "# df_final_top.corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pairs_df = pd.DataFrame(data = pairs, columns=['P1','P2'])\n",
    "# pairs_df.to_csv('/Users/kevin/Desktop/OMSA/CSE6242/Project/Project Visual/high_corr_vars.csv')\n",
    "\n",
    "\n",
    "#plt.subplot(1, 3, 1) # row 1, col 2 index 1\n",
    "# plt.scatter(df_final['LBXMCHSI'], df_final['LBXMCVSI'])\n",
    "\n",
    "# plt.subplot(1, 3, 2) # index 2\n",
    "# plt.scatter(df_final['LBDSBUSI'], df_final['LBXSBU'])\n",
    "\n",
    "# plt.subplot(1, 3, 3) # index 2\n",
    "# plt.scatter(df_final['PHAALCMN'], df_final['PHAANTMN'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.5 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
